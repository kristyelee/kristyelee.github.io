<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the  Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->

<html lang="en">

<head>
    <title>CS 194-26 Project 5: Kristy Lee</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
    <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css"> -->
    <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script> -->
    <!-- <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css'>-->
</head>


<body>
    <!-- Page Content -->
    <div class="container">
        <div class="row">
            <!-- Entries Column -->
            <div class="col-md-8" style="min-height: 100vh; height: auto;">
                <!-- <img class="img-responsive" src="banner.jpg" alt=""><br> -->
                <h1>CS 194-26 Project 5 Report: Facial Keypoint Detection with Neural Networks</h1>
                <div class="col-md-4">
                    <h2><p>Kristy Lee</p></h2>
                    <p><b>kristylee@berkeley.edu</b></p>
                    <p>
                        This project involves working on deep learning code where neural networks are used to detect keypoints. I create various datasets that consist of image data and corresponding keypoints. 
                    </p>
                </div>
                <div style="margin-top:3%; text-align:left;">
                    <h2>Part 1: Nose Tip Detection</h2>
                    <p>
                        The components of this part of the report are:
                    </p>
                    <ul>
                        <li>
                            Show samples of data loader (5 points)
                        </li>
                        <li>
                            Plot train and validation loss (5 points)
                        </li>
                        <li>
                            Show how hyper parameters affect results (5 points)
                        </li>
                        <li>
                            Show 2 success/failure cases (5 points)
                        </li>
                    </ul>
                    <h3>Show Samples of Data Loader (5 points)</h3>
                    <p>I split the IMM Face Database Dataset into a training dataset and a validation dataset: all images corresponding to indices 1-32 (32 * 6 = 192 images) become part of the training set, and all images corresponding to indices 33-40 (8 * 6 = 48 images) correspond to the validation set. For each image, I convert the image to grayscale, normalize the image pixel values from 0 to 255 using <code>image.astype(np.float32) / 255 - 0.5</code> to get normalized values -0.5 to 0.5, and resize the image to 80 x 60. As the "labels" of these datasets, we load the keypoints for each image and use the nose keypoints as the labels. After creating the datasets, we create the respective dataloaders. For the training dataloader, I use batch size 10, and for the validation dataloader, I used batch size 8. Here are a few example images from the datasets: </p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/sample_image_1.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 1</figcaption> 
                            </td>
                            <td>
                                <img src="images_5/sample_image_2.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 2</figcaption> 
                            </td>
                        </tr><tr>
                            <td>
                                <img src="images_5/sample_image_3.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 3</figcaption> 
                            </td>
                            <td>
                                <img src="images_5/sample_image_4.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 4</figcaption> 
                            </td>
                        </tr>
                    </table>
                    <h3>Plot Train and Validation Loss (5 points)</h3>
                    <p>
                        Here is the model architecture I used to create a classifier that predicts nosepoints on the validation set. It consists of 3 convolutional 2d layers, relu, and max pool layers.
                    </p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/cnn_part_1.png" alt="Whiteboard1" style="width:500px;height:217px;"/>
                                <figcaption align="center">CNN Architecture</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/cnn_d_part_1.png" alt="Whiteboard1" style="width:450px;height:311px;"/>
                                <figcaption align="center">CNN Architecture</figcaption> 
                            </td>
                        </tr>

                    </table>
                    <p>
                        I train the model for 25 epochs using a learning rate of 0.001 and use MSE loss to measure model performance. I plot training and validation losses versus number of epochs. Here are the graphs below:
                    </p>

                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/part1_training_loss.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">Training MSE Loss</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/part1_validation_loss.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">Validation MSE Loss</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/part1_loss.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">MSE Loss</figcaption> 
                            </td>
                        </tr>
                    </table>


                    <h3>Show How Hyperparameters Affect Results (5 points)</h3>
                    <p>
                        I vary two hyperparameters: learning rate and number of layers. The first hyperparameter I vary is learning rate (changed to 0.0005). In light of that, the MSE loss graphs are below. 
                    </p>

                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/part1_training_loss_LR.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">Training MSE Loss(LR)</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/part1_validation_loss_LR.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">Validation MSE Loss(LR)</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/part1_loss_LR.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">MSE Loss(LR)</figcaption> 
                            </td>
                        </tr>
                    </table>

                    <p>The training and validation losses after varying the learning rate is slightly less than in the original case.</p>

                    <p>The second hyperparameter I tried varying was the number of layers in the network. I used 4 layers instead of 3 layers here. </p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/cnn-adjusting-layers.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">New CNN Architecture</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/part1_training_loss_layers.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">Training MSE Loss(Layers)</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/part1_validation_loss_layers.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">Validation MSE Loss(Layers)</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/part1_loss_layers.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">MSE Loss(Layers)</figcaption> 
                            </td>
                        </tr>
                    </table>

                    <p>The loss graphs end with slightly higher value than the loss graphs that only trained a CNN with 3 layers. </p>

                    <h3>Show 2 Success/Failure Cases (5 points) </h3>
                    <p>Here are two success and failure cases:</p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/image_0_0.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Success 1</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/image_4_2.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Success 2</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/image_4_0.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Failure 1</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/image_4_7.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Failure 2</figcaption> 
                            </td>
                        </tr>
                    </table>
                    <p>I think these cases fail because the men are facing the side instead of front, which makes the nose harder to detect.</p>

                    <h2>Part 2: Full Facial Keypoints Detection</h2>
                    <p>The components of this report are:</p>
                    <ul>
                        <li>
                            Show samples of data loader (5 points)
                        </li>
                        <li>
                            Report detailed architecture (5 points)
                        </li>
                        <li>
                            Plot train and validation loss (5 points)
                        </li>
                        <li>
                            Show 2 success/failure cases (5 points)
                        </li>
                        <li>
                            Visualize learned features (5 points)
                        </li>
                    </ul>
                    <p>
                        Instead of just the nosepoint now, in this section we're going to try to detect full facial keypoints. In order to improve the accuracy of the detections, for each image I use data augmentation that involves picking out a random rotation between [-15, 15] degrees and a translation between [-10, 10] in both axes and apply these transformations to the image and the corresponding keypoints. 
                    </p>

                    <h3>Show Samples of Data Loader (5 points)</h3>
                    <p>Similar to last time, I split the IMM Face Database Dataset into a training dataset and a validation dataset: all images corresponding to indices 1-32 (32 * 6 = 192 images) become part of the training set, and all images corresponding to indices 33-40 (8 * 6 = 48 images) correspond to the validation set. For each image, I apply data augmentation, convert the image to grayscale, normalize the image pixel values from 0 to 255 using <code>image.astype(np.float32) / 255 - 0.5</code> to get normalized values -0.5 to 0.5, and resize the image to 240 x 180. As the "labels" of these datasets, we load the keypoints for each image and use the nose keypoints as the labels. After creating the datasets, we create the respective dataloaders. For the training dataloader, I use batch size 10, and for the validation dataloader, I used batch size 8. Here are a few example images from the datasets: </p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/sample_image_full_facial_keypoints_1.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 1</figcaption> 
                            </td>
                            <td>
                                <img src="images_5/sample_image_full_facial_keypoints_2.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 2</figcaption> 
                            </td>
                        </tr><tr>
                            <td>
                                <img src="images_5/sample_image_full_facial_keypoints_3.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 3</figcaption> 
                            </td>
                            <td>
                                <img src=images_5/sample_image_full_facial_keypoints_4.png alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 4</figcaption> 
                            </td>
                        </tr>
                    </table>


                    <h3>Report Detailed Architecture (5 points)</h3>
                    <p>Here is the detailed CNN architecture I used for the full facial keypoints. I use 5 convolutional layers: </p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/cnn-full-facial-keypoints.png" alt="Whiteboard1" style="width:500px;height:266px;" />
                                <figcaption align="center">CNN Full Facial Keypoints</figcaption> 
                            </td>
                        </tr><tr>
                            <td>
                                <img src="images_5/cnn-full-facial-keypoints-pt2.png" alt="Whiteboard1" style="width:500px;height:386px;" />
                                <figcaption align="center">CNN Full Facial Keypoints Pt 2</figcaption> 
                            </td>
                        </tr>
                        
                        
                        
                    </table>
                    <h3>Plot Train and Validation Loss (5 points)</h3>
                        <p>
                            I train the model for 25 epochs using a learning rate of 0.001 and use MSE loss to measure model performance. I plot training and validation losses versus number of epochs. Here are the graphs below:
                        </p>
                        
                        <table align="center">
                            <tr>
                                <td>
                                    <img src="images_5/part2_training_loss.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                    <figcaption align="center">Training MSE Loss</figcaption> 
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <img src="images_5/part2_validation_loss.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                    <figcaption align="center">Validation MSE Loss</figcaption> 
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <img src="images_5/part2_loss.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                    <figcaption align="center">MSE Loss</figcaption> 
                                </td>
                            </tr>
                        </table>

                    <h3>Show 2 Success/Failure Cases (5 points))</h3>
                    <p>Here are two success and two failure cases: </p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/image_2_3_2.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Success 1</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/image_3_7_2.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Success 2</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/image_1_1_2.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Failure 1</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/image_5_4_2.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Failure 2</figcaption> 
                            </td>
                        </tr>
                    </table>

                    <p>Again the reason why I believe the network fails in the cases of these images is because the men are facing sideways. </p>

                    <h3>Visualize Learned Features (5 points)</h3>
                    <p>Here are the first 15 visualized filters for each layers:</p>
                    
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/layer_0.png" alt="Whiteboard1" style="width:864px;height:144px;" />
                                <figcaption align="center">CNN Layer 0</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/layer_3.png" alt="Whiteboard1" style="width:847px;height:142px;" />
                                <figcaption align="center">CNN Layer 3</figcaption> 
                            </td>
                        </tr>

                        <tr>
                            <td>
                                <img src="images_5/layer_5.png" alt="Whiteboard1" style="width:849px;height:148px;" />
                                <figcaption align="center">CNN Layer 5</figcaption> 
                            </td>
                        </tr>

                        <tr>
                            <td>
                                <img src="images_5/layer_8.png" alt="Whiteboard1" style="width:850px;height:155px;" />
                                <figcaption align="center">CNN Layer 8</figcaption> 
                            </td>
                        </tr>

                        <tr>
                            <td>
                                <img src="images_5/layer_10.png" alt="Whiteboard1" style="width:852px;height:166px;" />
                                <figcaption align="center">CNN Layer 10</figcaption> 
                            </td>
                        </tr>
                    </table>

                    <h2>Part 3: Train With Larger Dataset</h2>
                    <p>The components of this report are:</p>
                    <ul>
                        <li>
                            Submit a working model to Kaggle competition (15 points)
                        </li>
                        <li>
                            Report detailed architecture (10 points)
                        </li>
                        <li>
                            Plot train and validation loss (10 points)
                        </li>
                        <li>
                            Visualize results on test set (10 points)
                        </li>
                        <li>
                            Run on at least 3 of your own photos (10 points)
                        </li>
                    </ul>

                    <h3>Submit a working model to Kaggle competition (15 points)</h3>
                    <p>
                        My submission to Kaggle is under the name "Kristy Lee", and my MAE (mean absolute error) is 30.55390. I didn't use data augmentation for the test dataloader, and I converted predicted keypoints to absolute pixel coordinate. For the training and validation dataloaders, I used a batch size of 30.
                    </p>
                    <h3>Example iBug Training Dataset Images</h3>
                    <p>Here are some example iBug traiing set images. For each image of the dataset, I applied data augmentation of a random rotation between [-15, 15] degrees, followed by a random translation between [-10, 10]. </p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/sample_image_ibug_1.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 1</figcaption> 
                            </td>
                            <td>
                                <img src="images_5/sample_image_ibug_2.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 2</figcaption> 
                            </td>
                        </tr><tr>
                            <td>
                                <img src="images_5/sample_image_ibug_3.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 3</figcaption> 
                            </td>
                            <td>
                                <img src="images_5/sample_image_ibug_4.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Example Picture 4</figcaption> 
                            </td>
                        </tr>
                    </table>
                    <h3>Report Detailed Architecture (10 points)</h3>
                    <p>
                        I used the ResNet18 CNN architecture, with the following changes: 1) Because the dataloader returns grayscale images of dimension (1, 224, 224), I changed the first layer of the model to have input channel of 1. 2) For the output of the model, since we want to return 68 * 2 predictions for 68 landmarks, I change the last layer's out channels from 1000 to 68 * 2 = 136. I uploaded the checkpoint for this model to Gradescope. Here's the PyTorch summary of the architecture:
                    </p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/resnet18-1.png" alt="Whiteboard1" style="width:500px;height:332px;"/>
                                <figcaption align="center">ResNet18</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/resnet18-2.png" alt="Whiteboard1" style="width:500px;height:342px;"/>
                                <figcaption align="center">ResNet18</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/resnet18-3.png" alt="Whiteboard1" style="width:500px;height:108px;"/>
                                <figcaption align="center">ResNet18</figcaption> 
                            </td>
                        </tr>
                    </table>
                
                    <h3>Plot Train and Validation Loss (10 points)</h3>
                    <p>
                        I train the model for 20 epochs using a learning rate of 0.001 and use MSE loss to measure model performance. I plot training and validation losses versus number of epochs. Here are the graphs below:
                    </p>

                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/part3_training_loss.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">Training MSE Loss</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/part3_validation_loss.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">Validation MSE Loss</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/part3_loss.png" alt="Whiteboard1" style="width:500px;height:300px;"/>
                                <figcaption align="center">MSE Loss</figcaption> 
                            </td>
                        </tr>
                    </table>

                    <h3>Visualize Results on Test Set (10 points)</h3>
                    <p>Here are some results of keypoints on the test set images (note - no data augmentation was applied here): </p>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/image_946.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Success 1</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/image_984.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Success 2</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/image_963.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Failure 1</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/image_973.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Failure 2</figcaption> 
                            </td>
                        </tr>
                    </table>
                    <p>I think the first failed case fails because the girl's head is tilted sideways which make the exact position of the chin, eyebrows, eyes harder to predict. For the second failed image I think this fails because the girl's chin is too close to her mouth in the image and the model may be more used to a particular proportion of distance between a person's mouth and chin.</p>
                    <h3>Run on at least 3 of Your Own Photos (10 points)</h3>
                    <table align="center">
                        <tr>
                            <td>
                                <img src="images_5/my_image_0.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Kristy</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/my_image_1.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Barack Obama</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/my_image_2.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Joe Biden</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/my_image_3.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">Andrew Ng</figcaption> 
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <img src="images_5/my_image_4.png" alt="Whiteboard1" style="width:432px;height:288px;" />
                                <figcaption align="center">George Bush</figcaption> 
                            </td>
                        </tr>
                        
                    </table>
                    <p>Despite the success in identifying facial keypoints on the test set (see https://drive.google.com/drive/folders/1q_uJ5uoZEYCEmBwOBK98ayUulRGSppr6?usp=sharing for mostly successful images!), I've used multiple photos and all seem to have predicted keypoints for eyes and eyebrows that are slightly off. The Obama photo is the most successful image with keypoint predictions, with only the eyes + eyebrow keypoints above. My face (Kristy) has incorrectly predicted keypoints, probably because my face largely dominated the image (so the points fell inside my face rather than around) and is more vertical/skinny face than a round face. Joe Biden's face has keypoints that acurately fit the chin/jawline, but the eye and eyebrow keypoints are again slightly above. Andrew Ng's photo has keypoints that are tilted at an angle relative to the face, making it not a good fit.  George Bush's photo has keypoints that fit the jawline and chin but the eye and eyebrow keypoints are farther above.</p>

                    <h3>What I Learned</h3>
                    <p>
                        I thought that the coolest thing I learned when doing this project part is constructing or reusing pretrained (+ edited) neural network models for predicting keypoints on faces on new images. I thought that experimenting with image resizing/cropping, data augmentation transformations for full facial keypoint detection and training with the iBug dataset, and learning about how to run epochs for the training loop of the model are all cool parts of the project that I appreciated learning more about. 
                    </p>

                    <h3>Acknowledgements</h3>
                    <ul>
                        <li>
                            <a href="https://inst.eecs.berkeley.edu/~cs194-26/fa21/hw/proj5/">CS194-26 Project 5 Description</a> 
                        </li>
                        <li>
                            I also want to acknowledge the multiple PyTorch guides I've referenced in writing the code for this project, as well as some other guides: https://learnopencv.com/image-rotation-and-translation-using-opencv/, https://stackoverflow.com/questions/34372480/rotate-point-about-another-point-in-degrees-python
                        </li>
                    </ul>
                </div>
            </div>


            

        </div>

    </div>

</body>


</html>
